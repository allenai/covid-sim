{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import csv\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertModel\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List, Dict\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import List\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import faiss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results.covid_dataset.all.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, sents = df[\"sentence_id\"].tolist(), df[\"sentence_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2318939"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sequences in bold indicate codons requiring multiple nucleotide substitutions .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These analytic weights were then multiplied by the appropriate adjustment factors as described above .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(object):\n",
    "    \n",
    "    def __init__(self, device = 'cpu'):\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('/media/shauli/Elements/current_projects/AI2/Clustering/scibert_scivocab_uncased/vocab.txt')\n",
    "        self.model = BertModel.from_pretrained('/media/shauli/Elements/current_projects/AI2/Clustering/scibert_scivocab_uncased/')\n",
    "            \n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "        self.device = device\n",
    "        self.pad_token = self.tokenizer.convert_tokens_to_ids([self.tokenizer.pad_token])[0]\n",
    "        \n",
    "    def tokenize_and_pad(self, texts: List[str]):\n",
    "        \n",
    "        indexed_texts = [self.tokenizer.encode(text, add_special_tokens=True, max_length = 512) for text in texts] #\n",
    "        max_len = max(len(text) for text in indexed_texts)\n",
    "        indexed_texts = [text + [self.pad_token] * (max_len - len(text)) for text in indexed_texts]\n",
    "        idx_tensor = torch.LongTensor(indexed_texts).to(self.device)\n",
    "        att_tensor = idx_tensor != self.pad_token\n",
    "        \n",
    "        return idx_tensor, att_tensor\n",
    "    \n",
    "    def encode(self, sentences: List[str], sentence_ids: List[str], batch_size: int, strategy: str = \"cls\", fname=\"\"):\n",
    "        assert len(sentences) == len(sentence_ids)\n",
    "        \n",
    "        with open(fname, \"w\", encoding = \"utf-8\") as f:\n",
    "            \n",
    "            for batch_idx in tqdm.tqdm_notebook(range(0, len(sentences), batch_size), total = len(sentences)//batch_size):\n",
    "            \n",
    "                batch_sents = sentences[batch_idx: batch_idx + batch_size]\n",
    "                batch_ids = sentence_ids[batch_idx: batch_idx + batch_size]\n",
    "                assert len(batch_sents) == len(batch_ids)\n",
    "                \n",
    "                idx, att_mask = self.tokenize_and_pad(batch_sents)\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(idx, attention_mask = att_mask)\n",
    "                    last_hidden = outputs[0]\n",
    "                \n",
    "                    if strategy == \"cls\":\n",
    "                        h = last_hidden[:, 0, ...]\n",
    "                    elif strategy == \"mean-cls\":\n",
    "                        h = torch.cat([last_hidden[:, 0, ...], torch.mean(last_hidden, axis = 1)], axis = 1)\n",
    "                    elif strategy == \"mean-cls-max\":\n",
    "                       h_max = torch.max(last_hidden, axis = 1).values\n",
    "                       h = torch.cat([last_hidden[:, 0, ...], torch.mean(last_hidden, axis = 1), h_max], axis = 1)\n",
    "                    elif strategy == \"mean\":\n",
    "                        h = torch.mean(last_hidden, axis = 1)\n",
    "                    elif strategy == \"median\":\n",
    "                        h = torch.median(last_hidden, axis = 1).values\n",
    "                    elif strategy == \"max\":\n",
    "                        h = torch.max(last_hidden, axis = 1).values\n",
    "                    elif strategy == \"min\":\n",
    "                        h = torch.min(last_hidden, axis = 1).values\n",
    "            \n",
    "                batch_np = h.detach().cpu().numpy()\n",
    "                assert len(batch_np) == len(batch_sents)\n",
    "                \n",
    "                sents_states_ids = zip(batch_sents, batch_np, batch_ids)\n",
    "                for sent, vec, sent_id in sents_states_ids:\n",
    "                    \n",
    "                    vec_str = \" \".join([\"%.4f\" % x for x in vec])\n",
    "                    sent_dict = {\"text\": sent, \"vec\": vec_str, \"id\": sent_id}\n",
    "                    f.write(json.dumps(sent_dict) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "encoder = BertEncoder(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9203ac0ad4ba47b6bed4bddd10b34ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=72466), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6c4c706a841d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mean-cls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"output.mean-cls.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-ae5f9cdd8562>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, sentence_ids, batch_size, strategy, fname)\u001b[0m\n\u001b[1;32m     51\u001b[0m                         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mbatch_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_np\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "h = encoder.encode(sents, ids, batch_size = 32, strategy = \"mean-cls-max\", fname=\"output.mean-cls-max.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50000\n",
    "data = []\n",
    "\n",
    "with open(\"output.mean-cls.jsonl\", \"r\", encoding = \"utf-8\") as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "            data_dict = eval(line)\n",
    "            data_dict[\"vec\"] = np.array([float(x) for x in data_dict[\"vec\"].split(\" \")]).astype(\"float32\")\n",
    "            data.append(data_dict)\n",
    "            i += 1\n",
    "            if i > n: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_sample = np.array([d[\"vec\"] for d in data])\n",
    "sents_sample =[d[\"text\"] for d in data]\n",
    "pca = PCA(n_components = 0.98)\n",
    "vecs_sample_pca = pca.fit_transform(vecs_sample).astype(\"float32\")\n",
    "\n",
    "index = faiss.IndexFlatIP(vecs_sample_pca.shape[1])\n",
    "index.add(np.ascontiguousarray(vecs_sample_pca/np.linalg.norm(vecs_sample_pca, axis = 1, keepdims = True)))\n",
    "faiss.write_index(index, \"index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = np.array([index.reconstruct(i) for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "spread_idx = np.array([i for i,sent in enumerate(sents_sample) if \"economic\" in sent])\n",
    "spread_mean = np.mean(vecs_sample[spread_idx], axis = 0)\n",
    "spread_mean_pca = pca.transform([spread_mean])\n",
    "D, I = index.search(np.ascontiguousarray(spread_mean_pca), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spread_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ 62 , 63 ] The economic condition may contribute to a false diagnosis of microcephaly , and postdiagnosis follow-up is necessary to make a definite diagnosis or complete correct diagnosis , considering the limitation of the operational definition in the size of head circumference .'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[spread_idx[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 . the current capacity of local health care systems to detect and respond to importation and transmission of EVD , and their variation across the region ( health care system characteristics ) ; and 4 . plausible interventions to support local health care systems to detect and respond to importation and transmission of EVD ( interventions ) .\n",
      "===========================================================\n",
      "This has reignited concerns about shifting transmission dynamics of communicable diseases , including how climate change is likely to affect their global spread , and it has reiterated the need for political action to meet challenges in the near future .\n",
      "===========================================================\n",
      "Vaccination of these animals would cost at least US$ 373 000 per year ( US$ 1.1 per vaccine ) ; a significant financial burden to small-scale farmers who rely on livestock sales to pay for house maintenance and childhood education and for local governments that have not previously been affected by rabies .\n",
      "===========================================================\n",
      "With depleted numbers of experienced health care workers in EVD-affected countries resulting from the high case fatality rate ( as high as 59 % ) , 111 affected countries will need regional and international support to augment health care delivery for citizens beyond providing support for those infected with EVD .\n",
      "===========================================================\n",
      "At the individual level , support is needed to help survivors resume normal lives and to address challenges such as emotional distress , health issues , loss of possessions , and difficulty regaining their livelihoods .\n",
      "===========================================================\n",
      "In light of the epidemic threat identified here , these cities should consider implementing more aggressive prevention policies as necessary , while respecting human rights and the dignity of affected individuals and of those who might be disadvantaged by stricter quarantine and control mechanisms .\n",
      "===========================================================\n",
      "Despite this , the devastating global burden of infectious diseases is far from being alleviated ; the recent outbreak of Ebola virus serves as a reminder of how infectious diseases continue to disproportionately affect the truly destitute in economically developing regions .\n",
      "===========================================================\n",
      "These outbreaks are associated with the relaxation of certain levels of hygiene control ; overcrowding of cities , which tends to perpetuate certain diseases ; the presence of factors that decrease the ability of each individual to confront pathogens ; and the growing mobility of the world 's population to locations where infectious diseases have not existed previously [ 18 ] , causing unexpected consequences for the entire global health system .\n",
      "===========================================================\n",
      "In this statement from the European Society of Intensive Care Medicine ( ESICM ) , we aim to describe some of the facts , the fears and the areas where scientific societies can help to improve our understanding of these problems and at the same time reduce the impact of these issues for our members and their patients .\n",
      "===========================================================\n",
      "Health partnerships between institutions in the developed and developing world are becoming increasingly important in combating the 21st century global health challenges outlined above but more recently the focus has shifted towards the need to strengthen weak health systems in low and middle income countries .\n",
      "===========================================================\n"
     ]
    }
   ],
   "source": [
    "for i in I.squeeze():\n",
    "    print(sents_sample[i])\n",
    "    print(\"===========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(np.random.rand(10,175).astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001, 175)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_sample_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
